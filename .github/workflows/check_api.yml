name: Check and Remove Archived URLs in Parallel

on:
  workflow_dispatch:  # Allow manual triggering

jobs:
  check-and-remove:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Check URLs and update file
      run: |
        import requests
        from multiprocessing import Pool, cpu_count

        def process_url(url):
            api_url = f"https://archive.org/wayback/available?url={url}"
            try:
                response = requests.get(api_url, timeout=10)
                data = response.json()
                if 'archived_snapshots' not in data or not data['archived_snapshots']:
                    return url
            except:
                return url  # Assume not archived if there's an error
            return None

        with open('output_urls.txt', 'r') as file:
            urls = file.read().splitlines()

        # Use 20 processes or the number of CPUs, whichever is smaller
        num_processes = min(20, cpu_count())
        
        with Pool(num_processes) as pool:
            new_urls = pool.map(process_url, urls)

        # Filter out None values (archived URLs)
        new_urls = [url for url in new_urls if url is not None]

        with open('output_urls.txt', 'w') as file:
            file.write('\n'.join(new_urls))

        print(f"Processed {len(urls)} URLs. Kept {len(new_urls)} URLs.")
      shell: python

    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add output_urls.txt
        git diff --quiet && git diff --staged --quiet || git commit -m "Remove already archived URLs from output_urls.txt"
        git push
