name: Check and Remove Archived URLs in Parallel

on:
  #schedule:
  #  - cron: '0 0 * * *'  # Run daily at midnight
  workflow_dispatch:  # Allow manual triggering

jobs:
  check-and-remove:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Check URLs and update file
      run: |
        import requests
        from multiprocessing import Pool
        import time
        
        def process_url(url):
            api_url = f"https://archive.org/wayback/available?url={url}"
            try:
                response = requests.get(api_url, timeout=10)
                data = response.json()
                if 'archived_snapshots' not in data or not data['archived_snapshots']:
                    return url
                else:
                    return None
            except:
                return url  # Assume not archived if there's an error
        
        def process_batch(urls):
            with Pool(50) as pool:
                new_urls = pool.map(process_url, urls)
            return [url for url in new_urls if url is not None]
        
        # Read all URLs
        with open('output_urls.txt', 'r') as file:
            all_urls = sorted(file.read().splitlines())
        
        new_all_urls = []
        total_processed = 0
        batch_size = 1000
        
        while total_processed < len(all_urls):
            batch = all_urls[total_processed:total_processed + batch_size]
            new_batch = process_batch(batch)
            new_all_urls.extend(new_batch)
            
            urls_deleted = len(batch) - len(new_batch)
            print(f"{urls_deleted}")
            
            total_processed += batch_size
        
        # Write the updated list back to the file
        with open('output_urls.txt', 'w') as file:
            file.write('\n'.join(new_all_urls))
      shell: python

    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add output_urls.txt
        git diff --quiet && git diff --staged --quiet || git commit -m "Remove already archived URLs from output_urls.txt"
        git push
