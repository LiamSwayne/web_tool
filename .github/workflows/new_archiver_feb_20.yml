name: Archive URLs

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual triggering
  push:  # Add push trigger

jobs:
  archive:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Git
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"

      - name: Create script
        run: |
          cat > archive_urls.sh << 'EOF'
          #!/bin/bash

          # Script to archive URLs using the Internet Archive API

          # Initialize variables
          PASTED_URLS_FILE="pasted_urls.txt"
          ARCHIVED_URLS_FILE="already_archived.txt"
          ARCHIVE_COUNT=0
          MAX_ARCHIVES=100
          RETRY_COUNT=10
          IA_ENDPOINT="https://web.archive.org/save/"
          USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36"

          # Function to commit changes with retries
          commit_changes() {
              local attempts=0
              while [ $attempts -lt $RETRY_COUNT ]; do
                  git pull
                  git add "$PASTED_URLS_FILE" "$ARCHIVED_URLS_FILE"
                  git commit -m "Update archived URLs"
                  
                  if git push; then
                      echo "Successfully committed and pushed changes."
                      return 0
                  else
                      echo "Push failed. Attempt $((attempts+1))/$RETRY_COUNT"
                      attempts=$((attempts+1))
                      sleep 5
                  fi
              done
              
              echo "Failed to push changes after $RETRY_COUNT attempts."
              return 1
          }

          # Function to sort files
          sort_files() {
              if [ -f "$PASTED_URLS_FILE" ]; then
                  sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
              fi
              
              if [ -f "$ARCHIVED_URLS_FILE" ]; then
                  sort -u "$ARCHIVED_URLS_FILE" -o "$ARCHIVED_URLS_FILE"
              fi
          }

          # Function to check if URL is already archived in IA
          check_archived() {
              local url=$1
              local cdx_url="https://web.archive.org/cdx/search/cdx?url=${url}&output=json"
              
              response=$(curl -s "$cdx_url")
              
              if [[ "$response" == "[]" || -z "$response" ]]; then
                  return 1  # Not archived
              else
                  return 0  # Already archived
              fi
          }

          # Function to archive a URL
          archive_url() {
              local url=$1
              
              echo "Archiving: $url"
              curl -s -A "$USER_AGENT" -X POST "${IA_ENDPOINT}${url}"
              
              # Add to archived URLs file
              echo "$url" >> "$ARCHIVED_URLS_FILE"
              
              # Increment counter
              ARCHIVE_COUNT=$((ARCHIVE_COUNT+1))
              
              # Sleep to avoid rate limiting
              sleep 5
          }

          # Function to recursively scrape mecabricks.com
          scrape_mecabricks() {
              local start_url="https://www.mecabricks.com"
              local temp_urls_file="temp_urls.txt"
              local visited_file="visited_urls.txt"
              local queue_file="queue_urls.txt"
              
              echo "Starting recursive scrape of mecabricks.com..."
              
              # Initialize queue with starting URL
              echo "$start_url" > "$queue_file"
              touch "$visited_file"
              
              # Process URLs in queue
              while [ -s "$queue_file" ] && [ $(wc -l < "$PASTED_URLS_FILE") -lt 1000 ]; do
                  # Get next URL from queue
                  current_url=$(head -n 1 "$queue_file")
                  sed -i '1d' "$queue_file"
                  
                  # Skip if already visited
                  if grep -q "^$current_url$" "$visited_file"; then
                      continue
                  fi
                  
                  echo "Scraping: $current_url"
                  
                  # Mark as visited
                  echo "$current_url" >> "$visited_file"
                  
                  # Download the page
                  curl -s -A "$USER_AGENT" "$current_url" > page.html
                  
                  # Extract URLs from page
                  grep -o 'https://www.mecabricks.com/[^"]*' page.html | 
                      grep -v '\.(jpg|jpeg|png|gif|css|js)$' > "$temp_urls_file"
                  
                  # Add URLs to queue if not visited
                  while read -r new_url; do
                      if ! grep -q "^$new_url$" "$visited_file" && ! grep -q "^$new_url$" "$queue_file"; then
                          echo "$new_url" >> "$queue_file"
                      fi
                      
                      # Add to pasted_urls if not already archived
                      if ! grep -q "^$new_url$" "$ARCHIVED_URLS_FILE" && ! grep -q "^$new_url$" "$PASTED_URLS_FILE"; then
                          echo "$new_url" >> "$PASTED_URLS_FILE"
                      fi
                  done < "$temp_urls_file"
                  
                  # Clean up
                  rm page.html
                  
                  # Sleep to avoid overloading the server
                  sleep 2
              done
              
              # Clean up
              rm -f "$temp_urls_file" "$visited_file" "$queue_file"
              
              # Sort the pasted URLs file
              sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
              
              echo "Added $(wc -l < "$PASTED_URLS_FILE") URLs to $PASTED_URLS_FILE"
          }

          # Main script execution

          # Create files if they don't exist
          touch "$PASTED_URLS_FILE"
          touch "$ARCHIVED_URLS_FILE"

          # Check if pasted_urls.txt is empty
          if [ ! -s "$PASTED_URLS_FILE" ]; then
              echo "No URLs found in $PASTED_URLS_FILE. Starting scrape of mecabricks.com..."
              scrape_mecabricks
          fi

          # Process URLs from pasted_urls.txt until we reach MAX_ARCHIVES
          while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
              # Read the first URL from the file
              url=$(head -n 1 "$PASTED_URLS_FILE")
              
              # Remove the URL from pasted_urls.txt
              sed -i '1d' "$PASTED_URLS_FILE"
              
              # Check if URL is already in already_archived.txt
              if grep -q "^$url$" "$ARCHIVED_URLS_FILE" 2>/dev/null; then
                  echo "URL already in $ARCHIVED_URLS_FILE: $url"
                  continue
              fi
              
              # Check if URL is already archived in IA
              if check_archived "$url"; then
                  echo "URL already archived in Internet Archive: $url"
                  echo "$url" >> "$ARCHIVED_URLS_FILE"
              else
                  # Archive the URL
                  archive_url "$url"
              fi
              
              # If we've reached 20 URLs, commit changes
              if [ $((ARCHIVE_COUNT % 20)) -eq 0 ]; then
                  sort_files
                  commit_changes
              fi
          done

          # If pasted_urls.txt is empty and we haven't reached max archives, scrape mecabricks
          if [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ]; then
              echo "Need more URLs to reach target of $MAX_ARCHIVES. Starting scrape of mecabricks.com..."
              scrape_mecabricks
              
              # Continue processing the newly scraped URLs
              while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
                  url=$(head -n 1 "$PASTED_URLS_FILE")
                  sed -i '1d' "$PASTED_URLS_FILE"
                  
                  if grep -q "^$url$" "$ARCHIVED_URLS_FILE" 2>/dev/null; then
                      echo "URL already in $ARCHIVED_URLS_FILE: $url"
                      continue
                  fi
                  
                  if check_archived "$url"; then
                      echo "URL already archived in Internet Archive: $url"
                      echo "$url" >> "$ARCHIVED_URLS_FILE"
                  else
                      archive_url "$url"
                  fi
                  
                  if [ $((ARCHIVE_COUNT % 20)) -eq 0 ]; then
                      sort_files
                      commit_changes
                  fi
              done
          fi

          # Final sort and commit
          sort_files
          commit_changes

          echo "Completed archiving. Total new URLs archived: $ARCHIVE_COUNT"
          EOF
          
          chmod +x archive_urls.sh

      - name: Run archiving script
        run: ./archive_urls.sh

      - name: Push any remaining changes
        run: |
          git add pasted_urls.txt already_archived.txt
          git commit -m "Final update of archived URLs" || echo "No changes to commit"
          git push || echo "No changes to push"