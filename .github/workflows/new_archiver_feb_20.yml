name: Archive URLs (Feb 20)
on:
 workflow_dispatch:
 schedule:
   - cron: '0 * * * *'  # Run every hour
 push:
   paths:
     - 'pasted_urls.txt'
jobs:
 archive:
   runs-on: ubuntu-latest
   
   steps:
     - uses: actions/checkout@v4
       with:
         token: ${{ secrets.GITHUB_TOKEN }}
     
     - name: Install xmllint
       run: sudo apt-get install -y libxml2-utils
     
     - name: Configure Git
       run: |
         git config user.name "GitHub Action"
         git config user.email "action@github.com"
     
     - name: Create files if they don't exist
       run: |
         touch pasted_urls.txt
         touch already_archived.txt
     
     - name: Archive URLs
       run: |
         # Ensure files exist and get hash
         touch pasted_urls.txt already_archived.txt
         original_hash=$(git hash-object pasted_urls.txt)
         
         # Create temporary files
         touch processed_urls.txt
         touch all_urls.txt
         
         # Copy initial URLs to all_urls (with error handling)
         if [ -s pasted_urls.txt ]; then
           grep -E "^https?://" pasted_urls.txt > all_urls.txt || true
         fi
         
         # Function to extract URLs from HTML/XML content
         extract_urls() {
           local content="$1"
           # Extract from href and src attributes
           echo "$content" | grep -oP '(?<=href=")[^"]*|(?<=src=")[^"]*' | grep -E "^https?://" || true
           # Extract from XML
           echo "$content" | grep -oP 'https?://[^\s<>"'"'"')\]}]+' | grep -E "^https?://" || true
         }
         
         # First pass: collect URLs from HTML/XML pages
         echo "Crawling HTML/XML pages for additional URLs..."
         if [ -s all_urls.txt ]; then
           while read url; do
             if [[ $url =~ \.(html?|xml)$ ]] || [[ $(curl -sI "$url" | grep -i content-type) =~ text/html|text/xml|application/xml ]]; then
               echo "Crawling: $url"
               content=$(curl -s "$url")
               extract_urls "$content" >> all_urls.txt
             fi
           done < <(grep -E "^https?://" pasted_urls.txt || true)
         fi
         
         # Remove duplicates and sort
         if [ -s all_urls.txt ]; then
           sort -u all_urls.txt -o all_urls.txt
         fi
         
         # Pick 1000 random URLs to archive (if we have any URLs)
         if [ -s all_urls.txt ]; then
           shuf -n 1000 all_urls.txt | while read url; do 
             echo "$url" >> processed_urls.txt
             
             if [[ $url == *"reddit.com"* && $url != *"old.reddit.com"* ]]; then
               old_url=$(echo "$url" | sed 's/reddit.com/old.reddit.com/')
               urls=("$url" "$old_url")
             else
               urls=("$url")
             fi
             
             for process_url in "${urls[@]}"; do
               # Skip if already archived previously
               if grep -Fxq "$process_url" already_archived.txt; then
                 echo "$(date): URL already archived previously: $process_url"
                 continue
               fi
               
               archived=$(curl -s "https://web.archive.org/cdx/search/cdx?url=$process_url&output=json&limit=1")
               
               if [ "$archived" = "[]" ]; then
                 echo "$(date): Archiving new URL: $process_url"
                 if curl -s "https://web.archive.org/save/$process_url" > /dev/null; then
                   echo "$process_url" >> already_archived.txt
                   sleep 2
                 fi
               else
                 echo "$(date): Skipping already archived URL: $process_url"
                 echo "$process_url" >> already_archived.txt
               fi
             done
           done
         fi
         echo "Archive check complete."
         
         # Sort already_archived.txt if it exists and has content
         if [ -s already_archived.txt ]; then
           sort -u already_archived.txt -o already_archived.txt
         fi
         
         # Check if original file has changed
         current_hash=$(git hash-object pasted_urls.txt)
         if [ "$original_hash" = "$current_hash" ]; then
           # Remove processed URLs and sort remaining ones if we have any
           if [ -s processed_urls.txt ] && [ -s all_urls.txt ]; then
             grep -vxFf processed_urls.txt all_urls.txt | sort -u > pasted_urls.txt
           fi
           
           # Clean up temp files
           rm -f processed_urls.txt all_urls.txt
           
           # Commit both files
           git add pasted_urls.txt already_archived.txt
           git commit -m "Update archived URLs and remaining URLs" && git push || echo "Skipping commit due to conflict"
         else
           echo "File has changed during execution, skipping commit"
         fi
