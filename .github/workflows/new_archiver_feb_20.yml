name: Archive URLs (Feb 22)
on:
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *'
  push:
    paths:
      - 'pasted_urls.txt'

jobs:
  archive:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Configure Git
        run: |
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
          
      - name: Install dependencies
        run: pip install beautifulsoup4 requests urllib3

      - name: Extract and Scrape URLs
        run: |
          python3 - <<EOF
          import re
          import sys
          import time
          import requests
          from bs4 import BeautifulSoup
          from collections import deque
          from urllib.parse import urljoin, urlparse
          
          def is_valid_url(url):
              try:
                  result = urlparse(url)
                  return all([result.scheme, result.netloc])
              except:
                  return False
          
          def extract_urls(filename):
              try:
                  with open(filename, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  url_pattern = r'https?://[^\s<>"\']+'
                  return sorted(set(re.findall(url_pattern, content)))
              except:
                  return []
          
          def scrape_mecabricks(target_count=100):
              # Load already archived URLs
              try:
                  with open('already_archived.txt', 'r', encoding='utf-8') as f:
                      already_archived = set(line.strip() for line in f)
              except FileNotFoundError:
                  already_archived = set()

              base_url = 'https://mecabricks.com'
              visited = set()
              queue = deque([base_url])
              found_urls = set()
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                  'Accept-Language': 'en-US,en;q=0.5',
                  'Connection': 'keep-alive',
              }

              def safe_ia_check(url):
                  try:
                      response = requests.get(
                          f'https://web.archive.org/cdx/search/cdx?url={url}&output=json&limit=1',
                          headers=headers,
                          timeout=10
                      )
                      return len(response.json()) == 0  # Return True if needs archiving
                  except:
                      return True  # Assume needs archiving if check fails

              while queue and len(found_urls) < target_count:
                  current_url = queue.popleft()
                  if current_url in visited:
                      continue
                  visited.add(current_url)
                  
                  print(f"Scraping: {current_url}")
                  
                  try:
                      response = requests.get(current_url, headers=headers, timeout=15)
                      response.raise_for_status()
                      
                      soup = BeautifulSoup(response.text, 'html.parser')
                      
                      # Find all links on page
                      for link in soup.find_all('a', href=True):
                          href = link['href']
                          full_url = urljoin(current_url, href)
                          
                          if not is_valid_url(full_url):
                              continue
                          if 'mecabricks.com' not in full_url:
                              continue
                          if full_url in visited or full_url in found_urls:
                              continue
                          if full_url in already_archived:
                              continue
                              
                          # Add to queue first to explore
                          queue.append(full_url)
                          
                          # Check if it's a model URL
                          if '/models/' in full_url and safe_ia_check(full_url):
                              found_urls.add(full_url)
                              print(f"Found new URL ({len(found_urls)}/{target_count}): {full_url}")
                              if len(found_urls) >= target_count:
                                  break
                      
                      time.sleep(2)
                      
                  except Exception as e:
                      print(f"Error scraping {current_url}: {str(e)}")
                      time.sleep(5)

              return list(found_urls)[:target_count]
          
          # First try to get URLs from file
          urls = extract_urls('pasted_urls.txt')
          
          # If no URLs found, scrape mecabricks.com
          if not urls:
              print("No URLs found in file, scraping mecabricks.com...")
              urls = scrape_mecabricks(100)
          
          # Write URLs to file
          with open('all_urls.txt', 'w', encoding='utf-8') as f:
              for url in urls:
                  f.write(f"{url}\n")
          
          print(f"Total URLs found: {len(urls)}")
          EOF

      - name: Archive URLs
        run: |
          # [Keep the existing Archive URLs step unchanged]
          # ... (same as previous version)
