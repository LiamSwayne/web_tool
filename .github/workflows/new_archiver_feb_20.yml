name: Archive URLs

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual triggering
  push:  # Add push trigger

jobs:
  archive:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Git
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"

      - name: Create script
        run: |
          cat > archive_urls.sh << 'EOF'
          #!/bin/bash

          # Script to archive URLs using the Internet Archive API

          # Initialize variables
          PASTED_URLS_FILE="pasted_urls.txt"
          ARCHIVED_URLS_FILE="archived_urls.txt"
          ARCHIVE_COUNT=0
          MAX_ARCHIVES=100
          RETRY_COUNT=10
          IA_ENDPOINT="https://web.archive.org/save/"
          USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36"

          # Function to commit changes with retries
          commit_changes() {
              local attempts=0
              while [ $attempts -lt $RETRY_COUNT ]; do
                  git pull
                  git add "$PASTED_URLS_FILE" "$ARCHIVED_URLS_FILE"
                  git commit -m "Update archived URLs"
                  
                  if git push; then
                      echo "Successfully committed and pushed changes."
                      return 0
                  else
                      echo "Push failed. Attempt $((attempts+1))/$RETRY_COUNT"
                      attempts=$((attempts+1))
                      sleep 5
                  fi
              done
              
              echo "Failed to push changes after $RETRY_COUNT attempts."
              return 1
          }

          # Function to sort files
          sort_files() {
              if [ -f "$PASTED_URLS_FILE" ]; then
                  sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
              fi
              
              if [ -f "$ARCHIVED_URLS_FILE" ]; then
                  sort -u "$ARCHIVED_URLS_FILE" -o "$ARCHIVED_URLS_FILE"
              fi
          }

          # Function to check if URL is already archived in IA
          check_archived() {
              local url=$1
              local cdx_url="https://web.archive.org/cdx/search/cdx?url=${url}&output=json"
              
              response=$(curl -s "$cdx_url")
              
              if [[ "$response" == "[]" || -z "$response" ]]; then
                  return 1  # Not archived
              else
                  return 0  # Already archived
              fi
          }

          # Function to archive a URL
          archive_url() {
              local url=$1
              
              echo "Archiving: $url"
              curl -s -A "$USER_AGENT" -X POST "${IA_ENDPOINT}${url}"
              
              # Add to archived URLs file
              echo "$url" >> "$ARCHIVED_URLS_FILE"
              
              # Increment counter
              ARCHIVE_COUNT=$((ARCHIVE_COUNT+1))
              
              # Sleep to avoid rate limiting
              sleep 5
          }

          # Function to recursively scrape mecabricks.com
          scrape_mecabricks() {
              local start_url="https://www.mecabricks.com"
              local temp_urls_file="temp_urls.txt"
              
              echo "Starting recursive scrape of mecabricks.com..."
              
              # Download the homepage
              curl -s -A "$USER_AGENT" "$start_url" > homepage.html
              
              # Extract URLs from homepage
              grep -o 'https://www.mecabricks.com/[^"]*' homepage.html | grep -v '\.(jpg|jpeg|png|gif|css|js)$' > "$temp_urls_file"
              
              # Add URLs to pasted_urls if not already in archived_urls
              while read -r url; do
                  if ! grep -q "^$url$" "$ARCHIVED_URLS_FILE" 2>/dev/null; then
                      echo "$url" >> "$PASTED_URLS_FILE"
                  fi
              done < "$temp_urls_file"
              
              # Clean up
              rm homepage.html "$temp_urls_file"
              
              # Sort the pasted URLs file
              sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
              
              echo "Added $(wc -l < "$PASTED_URLS_FILE") URLs to $PASTED_URLS_FILE"
          }

          # Main script execution

          # Create files if they don't exist
          touch "$PASTED_URLS_FILE"
          touch "$ARCHIVED_URLS_FILE"

          # Check if pasted_urls.txt is empty
          if [ ! -s "$PASTED_URLS_FILE" ]; then
              echo "No URLs found in $PASTED_URLS_FILE. Starting scrape of mecabricks.com..."
              scrape_mecabricks
          fi

          # Process URLs from pasted_urls.txt
          while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
              # Read the first URL from the file
              url=$(head -n 1 "$PASTED_URLS_FILE")
              
              # Remove the URL from pasted_urls.txt
              sed -i '1d' "$PASTED_URLS_FILE"
              
              # Check if URL is already in archived_urls.txt
              if grep -q "^$url$" "$ARCHIVED_URLS_FILE" 2>/dev/null; then
                  echo "URL already in archived_urls.txt: $url"
                  continue
              fi
              
              # Check if URL is already archived in IA
              if check_archived "$url"; then
                  echo "URL already archived in Internet Archive: $url"
                  echo "$url" >> "$ARCHIVED_URLS_FILE"
              else
                  # Archive the URL
                  archive_url "$url"
              fi
              
              # If we've reached 20 URLs, commit changes
              if [ $((ARCHIVE_COUNT % 20)) -eq 0 ]; then
                  sort_files
                  commit_changes
              fi
          done

          # If pasted_urls.txt is empty and we haven't reached max archives, scrape mecabricks
          if [ ! -s "$PASTED_URLS_FILE" ] && [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ]; then
              echo "No more URLs in $PASTED_URLS_FILE. Starting scrape of mecabricks.com..."
              scrape_mecabricks
              
              # Continue processing the newly scraped URLs
              while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
                  url=$(head -n 1 "$PASTED_URLS_FILE")
                  sed -i '1d' "$PASTED_URLS_FILE"
                  
                  if grep -q "^$url$" "$ARCHIVED_URLS_FILE" 2>/dev/null; then
                      echo "URL already in archived_urls.txt: $url"
                      continue
                  fi
                  
                  if check_archived "$url"; then
                      echo "URL already archived in Internet Archive: $url"
                      echo "$url" >> "$ARCHIVED_URLS_FILE"
                  else
                      archive_url "$url"
                  fi
                  
                  if [ $((ARCHIVE_COUNT % 20)) -eq 0 ]; then
                      sort_files
                      commit_changes
                  fi
              done
          fi

          # Final sort and commit
          sort_files
          commit_changes

          echo "Completed archiving. Total new URLs archived: $ARCHIVE_COUNT"
          EOF
          
          chmod +x archive_urls.sh

      - name: Run archiving script
        run: ./archive_urls.sh

      - name: Push any remaining changes
        run: |
          git add pasted_urls.txt archived_urls.txt
          git commit -m "Final update of archived URLs" || echo "No changes to commit"
          git push || echo "No changes to push"