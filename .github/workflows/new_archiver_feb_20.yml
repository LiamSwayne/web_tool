Here's the modified script that includes recursive scraping of mecabricks.com when no source URLs are found:
yamlCopyname: Archive URLs (Feb 22)
on:
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *'
  push:
    paths:
      - 'pasted_urls.txt'

jobs:
  archive:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Configure Git
        run: |
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
          
      - name: Install dependencies
        run: pip install beautifulsoup4 requests urllib3

      - name: Extract and Scrape URLs
        run: |
          python3 - <<EOF
          import re
          import sys
          import requests
          from bs4 import BeautifulSoup
          from collections import deque
          from urllib.parse import urljoin, urlparse
          
          def is_valid_url(url):
              try:
                  result = urlparse(url)
                  return all([result.scheme, result.netloc])
              except:
                  return False
          
          def extract_urls(filename):
              try:
                  with open(filename, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  url_pattern = r'https?://[^\s<>"\']+'
                  return sorted(set(re.findall(url_pattern, content)))
              except:
                  return []
          
          def scrape_mecabricks(target_count=500):
              base_url = 'https://mecabricks.com'
              visited = set()
              queue = deque([base_url])
              found_urls = set()
              headers = {'User-Agent': 'Mozilla/5.0 (compatible; ArchiveBot/1.0)'}
              
              while queue and len(found_urls) < target_count:
                  current_url = queue.popleft()
                  if current_url in visited:
                      continue
                      
                  visited.add(current_url)
                  print(f"Scraping: {current_url}")
                  
                  try:
                      response = requests.get(current_url, headers=headers, timeout=30)
                      if response.status_code != 200:
                          continue
                      
                      soup = BeautifulSoup(response.text, 'html.parser')
                      
                      # Extract all links
                      for link in soup.find_all('a', href=True):
                          href = link['href']
                          full_url = urljoin(current_url, href)
                          
                          # Only process mecabricks.com URLs
                          if not is_valid_url(full_url) or 'mecabricks.com' not in full_url:
                              continue
                          
                          found_urls.add(full_url)
                          if full_url not in visited:
                              queue.append(full_url)
                      
                      # Sleep to be nice to the server
                      time.sleep(2)
                      
                  except Exception as e:
                      print(f"Error scraping {current_url}: {str(e)}")
                  
                  # Update progress
                  print(f"Found {len(found_urls)} URLs so far")
              
              return list(found_urls)
          
          # First try to get URLs from file
          urls = extract_urls('pasted_urls.txt')
          
          # If no URLs found, scrape mecabricks.com
          if not urls:
              print("No URLs found in file, scraping mecabricks.com...")
              urls = scrape_mecabricks(500)
          
          # Write URLs to file
          with open('all_urls.txt', 'w', encoding='utf-8') as f:
              for url in urls:
                  f.write(f"{url}\n")
          EOF
      
      - name: Archive URLs
        run: |
          # Function to retry curl with backoff
          function retry_curl() {
            local url=$1
            local max_attempts=3
            local attempt=1
            local wait_time=5
            
            while [ $attempt -le $max_attempts ]; do
              response=$(curl -sS --max-time 30 "$url" 2>&1)
              if [ $? -eq 0 ]; then
                echo "$response"
                return 0
              fi
              echo "$(date): Attempt $attempt failed for $url. Retrying in ${wait_time}s..."
              sleep $wait_time
              wait_time=$((wait_time * 2))
              attempt=$((attempt + 1))
            done
            
            echo "error"
            return 1
          }
          
          # Get original hash
          original_hash=$(git hash-object pasted_urls.txt || echo "")
          
          # Initialize files
          touch already_archived.txt processed_urls.txt
          
          # Pick and process 1000 random URLs
          shuf -n 1000 all_urls.txt | while IFS= read -r url || [ -n "$url" ]; do 
            if [ -z "$url" ]; then
              continue
            fi
            
            echo "$url" >> processed_urls.txt
            
            if [[ "$url" == *"reddit.com"* && "$url" != *"old.reddit.com"* ]]; then
              old_url=$(echo "$url" | sed 's/reddit.com/old.reddit.com/')
              urls=("$url" "$old_url")
            else
              urls=("$url")
            fi
            
            for process_url in "${urls[@]}"; do
              if [ -z "$process_url" ]; then
                continue
              fi
              
              if grep -Fxq "$process_url" already_archived.txt 2>/dev/null; then
                echo "$(date): Already archived: $process_url"
                continue
              fi
              
              # Check archive status with retry
              archived=$(retry_curl "https://web.archive.org/cdx/search/cdx?url=$process_url&output=json&limit=1")
              if [ "$archived" = "error" ]; then
                echo "$(date): Failed all attempts to check archive status for: $process_url"
                echo "$process_url" >> failed_urls.txt
                continue
              fi
              
              if [ "$archived" = "[]" ]; then
                echo "$(date): Archiving: $process_url"
                if retry_curl "https://web.archive.org/save/$process_url" > /dev/null; then
                  echo "$process_url" >> already_archived.txt
                  sleep 5
                else
                  echo "$(date): Failed to archive after retries: $process_url"
                  echo "$process_url" >> failed_urls.txt
                fi
              else
                echo "$(date): Already in Internet Archive: $process_url"
                echo "$process_url" >> already_archived.txt
              fi
            done
          done || true
          
          # Update files if no conflicts
          current_hash=$(git hash-object pasted_urls.txt 2>/dev/null || echo "")
          if [ "$original_hash" = "$current_hash" ]; then
            sort -u already_archived.txt -o already_archived.txt 2>/dev/null || true
            
            # Only sort and add failed_urls.txt if it exists
            if [ -f failed_urls.txt ]; then
              sort -u failed_urls.txt -o failed_urls.txt 2>/dev/null || true
              git add failed_urls.txt
            fi
            
            if [ -s processed_urls.txt ]; then
              grep -vxFf processed_urls.txt all_urls.txt 2>/dev/null | sort -u > pasted_urls.txt || true
            fi
            
            git add pasted_urls.txt already_archived.txt
            git commit -m "Update archived URLs and remaining URLs" && git push || echo "Skipping commit due to conflict"
          fi
          
          # Cleanup
          rm -f processed_urls.txt all_urls.txt
