name: Archive URLs

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual triggering
  push:  # Add push trigger

jobs:
  archive:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Git
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"

      - name: Create script
        run: |
          cat > archive_urls.sh << 'EOF'
          #!/bin/bash

          # Script to archive URLs using the Internet Archive API

          # Initialize variables
          PASTED_URLS_FILE="pasted_urls.txt"
          ALREADY_ARCHIVED_FILE="already_archived.txt"
          ARCHIVE_COUNT=0
          NEW_URL_COUNT=0
          MAX_ARCHIVES=100
          RETRY_COUNT=10
          IA_ENDPOINT="https://web.archive.org/save/"
          USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36"

          # Function to commit changes with retries
          commit_changes() {
              local attempts=0
              while [ $attempts -lt $RETRY_COUNT ]; do
                  git pull
                  git add "$PASTED_URLS_FILE" "$ALREADY_ARCHIVED_FILE"
                  git commit -m "Update archived URLs (New: $NEW_URL_COUNT, Total Archived: $ARCHIVE_COUNT)"
                  
                  if git push; then
                      echo "Successfully committed and pushed changes."
                      return 0
                  else
                      echo "Push failed. Attempt $((attempts+1))/$RETRY_COUNT"
                      attempts=$((attempts+1))
                      sleep 5
                  fi
              done
              
              echo "Failed to push changes after $RETRY_COUNT attempts."
              return 1
          }

          # Function to sort files
          sort_files() {
              if [ -f "$PASTED_URLS_FILE" ]; then
                  sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
              fi
              
              if [ -f "$ALREADY_ARCHIVED_FILE" ]; then
                  sort -u "$ALREADY_ARCHIVED_FILE" -o "$ALREADY_ARCHIVED_FILE"
              fi
          }

          # Function to check if URL is already archived in IA
          check_archived() {
              local url=$1
              local cdx_url="https://web.archive.org/cdx/search/cdx?url=${url}&output=json"
              
              response=$(curl -s "$cdx_url")
              
              if [[ "$response" == "[]" || -z "$response" ]]; then
                  return 1  # Not archived
              else
                  return 0  # Already archived
              fi
          }

          # Function to archive a URL
          archive_url() {
              local url=$1
              
              echo "Archiving: $url"
              curl_response=$(curl -s -A "$USER_AGENT" -X POST "${IA_ENDPOINT}${url}")
              echo "Archive result: $(echo "$curl_response" | head -n 10 | grep -v "html")"
              
              # Add to already archived URLs file
              echo "$url" >> "$ALREADY_ARCHIVED_FILE"
              
              # Increment counters
              ARCHIVE_COUNT=$((ARCHIVE_COUNT+1))
              NEW_URL_COUNT=$((NEW_URL_COUNT+1))
              
              echo "Total new URLs archived: $NEW_URL_COUNT"
              
              # Sleep to avoid rate limiting
              sleep 5
          }

          # Function to recursively scrape mecabricks.com
          scrape_mecabricks() {
              local start_url="https://www.mecabricks.com"
              local temp_urls_file="temp_urls.txt"
              local visited_urls_file="visited_urls.txt"
              
              # Create visited URLs file if it doesn't exist
              touch "$visited_urls_file"
              
              echo "Starting recursive scrape of mecabricks.com..."
              
              # Download the homepage if not visited yet
              if ! grep -q "^$start_url$" "$visited_urls_file" 2>/dev/null; then
                  echo "Visiting: $start_url"
                  curl -s -A "$USER_AGENT" "$start_url" > homepage.html
                  echo "$start_url" >> "$visited_urls_file"
                  
                  # Extract URLs from homepage
                  grep -o 'https://www.mecabricks.com/[^"]*' homepage.html | grep -v '\.(jpg|jpeg|png|gif|css|js)$' > "$temp_urls_file"
                  
                  # Add URLs to pasted_urls if not already archived
                  while read -r url; do
                      if ! grep -q "^$url$" "$ALREADY_ARCHIVED_FILE" 2>/dev/null && ! grep -q "^$url$" "$PASTED_URLS_FILE" 2>/dev/null; then
                          echo "$url" >> "$PASTED_URLS_FILE"
                      fi
                  done < "$temp_urls_file"
                  
                  # Clean up
                  rm homepage.html "$temp_urls_file"
                  
                  # Sort the pasted URLs file
                  sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
                  
                  echo "Added $(wc -l < "$PASTED_URLS_FILE") URLs to $PASTED_URLS_FILE"
              fi
              
              # If we still need URLs, go deeper (recursively)
              if [ ! -s "$PASTED_URLS_FILE" ] || [ $(wc -l < "$PASTED_URLS_FILE") -lt 50 ]; then
                  # Get next URL to visit for deeper crawling
                  for next_url in $(grep -o 'https://www.mecabricks.com/[^"]*' "$visited_urls_file" | head -n 10); do
                      if ! grep -q "^$next_url$" "$visited_urls_file" 2>/dev/null; then
                          echo "Deep crawling: $next_url"
                          curl -s -A "$USER_AGENT" "$next_url" > page.html
                          echo "$next_url" >> "$visited_urls_file"
                          
                          # Extract URLs from this page
                          grep -o 'https://www.mecabricks.com/[^"]*' page.html | grep -v '\.(jpg|jpeg|png|gif|css|js)$' > "$temp_urls_file"
                          
                          # Add URLs to pasted_urls if not already archived
                          while read -r url; do
                              if ! grep -q "^$url$" "$ALREADY_ARCHIVED_FILE" 2>/dev/null && ! grep -q "^$url$" "$PASTED_URLS_FILE" 2>/dev/null; then
                                  echo "$url" >> "$PASTED_URLS_FILE"
                              fi
                          done < "$temp_urls_file"
                          
                          # Clean up
                          rm page.html "$temp_urls_file"
                          
                          # Sort the pasted URLs file
                          sort -u "$PASTED_URLS_FILE" -o "$PASTED_URLS_FILE"
                          
                          echo "After deep crawl, now have $(wc -l < "$PASTED_URLS_FILE") URLs in $PASTED_URLS_FILE"
                          
                          # If we have enough URLs, break
                          if [ $(wc -l < "$PASTED_URLS_FILE") -ge 200 ]; then
                              break
                          fi
                      fi
                  done
              fi
              
              # Clean up
              rm -f "$visited_urls_file"
          }

          # Main script execution

          # Create files if they don't exist
          touch "$PASTED_URLS_FILE"
          touch "$ALREADY_ARCHIVED_FILE"

          # Check if pasted_urls.txt is empty
          if [ ! -s "$PASTED_URLS_FILE" ]; then
              echo "No URLs found in $PASTED_URLS_FILE. Starting scrape of mecabricks.com..."
              scrape_mecabricks
          fi

          # Process URLs from pasted_urls.txt
          while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
              # Read the first URL from the file
              url=$(head -n 1 "$PASTED_URLS_FILE")
              
              # Remove the URL from pasted_urls.txt
              sed -i '1d' "$PASTED_URLS_FILE"
              
              # Check if URL is already in already_archived.txt
              if grep -q "^$url$" "$ALREADY_ARCHIVED_FILE" 2>/dev/null; then
                  echo "URL already in $ALREADY_ARCHIVED_FILE: $url"
                  continue
              fi
              
              # Check if URL is already archived in IA
              if check_archived "$url"; then
                  echo "URL already archived in Internet Archive: $url"
                  echo "$url" >> "$ALREADY_ARCHIVED_FILE"
              else
                  # Archive the URL
                  archive_url "$url"
              fi
              
              # If we've reached 20 URLs, commit changes
              if [ $((ARCHIVE_COUNT % 20)) -eq 0 ] && [ $ARCHIVE_COUNT -gt 0 ]; then
                  sort_files
                  commit_changes
              fi
          done

          # If pasted_urls.txt is empty and we haven't reached max archives, scrape mecabricks
          while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ]; do
              if [ ! -s "$PASTED_URLS_FILE" ]; then
                  echo "No more URLs in $PASTED_URLS_FILE. Starting scrape of mecabricks.com..."
                  scrape_mecabricks
                  
                  # If we still don't have URLs, break to avoid infinite loop
                  if [ ! -s "$PASTED_URLS_FILE" ]; then
                      echo "No URLs found after scraping. Exiting."
                      break
                  fi
              fi
              
              # Process the newly scraped URLs
              while [ $ARCHIVE_COUNT -lt $MAX_ARCHIVES ] && [ -s "$PASTED_URLS_FILE" ]; do
                  url=$(head -n 1 "$PASTED_URLS_FILE")
                  sed -i '1d' "$PASTED_URLS_FILE"
                  
                  if grep -q "^$url$" "$ALREADY_ARCHIVED_FILE" 2>/dev/null; then
                      echo "URL already in $ALREADY_ARCHIVED_FILE: $url"
                      continue
                  fi
                  
                  if check_archived "$url"; then
                      echo "URL already archived in Internet Archive: $url"
                      echo "$url" >> "$ALREADY_ARCHIVED_FILE"
                  else
                      archive_url "$url"
                  fi
                  
                  if [ $((ARCHIVE_COUNT % 20)) -eq 0 ] && [ $ARCHIVE_COUNT -gt 0 ]; then
                      sort_files
                      commit_changes
                  fi
              done
          done

          # Final sort and commit
          sort_files
          commit_changes

          echo "Completed archiving. Total new URLs archived: $NEW_URL_COUNT (Total processed: $ARCHIVE_COUNT)"
          EOF
          
          chmod +x archive_urls.sh

      - name: Run archiving script
        run: ./archive_urls.sh

      - name: Push any remaining changes
        run: |
          git add pasted_urls.txt already_archived.txt
          git commit -m "Final update of archived URLs" || echo "No changes to commit"
          git push || echo "No changes to push"