name: Archive URLs

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual triggering
  push:  # Add push trigger
  
jobs:
  archive-urls:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Run archiving script
        run: |
          python - << 'EOF'
          import os
          import time
          import requests
          import subprocess
          from bs4 import BeautifulSoup
          import json
          import random
          
          # Create files if they don't exist
          if not os.path.exists('archived_urls.txt'):
              open('archived_urls.txt', 'a').close()
          if not os.path.exists('pasted_urls.txt'):
              open('pasted_urls.txt', 'a').close()
          
          # Load existing archived URLs
          with open('archived_urls.txt', 'r') as f:
              archived_urls = set(line.strip() for line in f if line.strip())
          print(f"Loaded {len(archived_urls)} previously archived URLs")
          
          # Load URLs from pasted_urls.txt
          with open('pasted_urls.txt', 'r') as f:
              pasted_urls = [line.strip() for line in f if line.strip()]
          print(f"Loaded {len(pasted_urls)} URLs from pasted_urls.txt")
          
          # Function to check if URL is already archived in Internet Archive
          def is_archived(url):
              try:
                  response = requests.get(f"https://archive.org/wayback/available?url={url}", timeout=15)
                  data = response.json()
                  has_snapshots = bool(data.get('archived_snapshots', {}))
                  print(f"API check for {url}: {'Already archived' if has_snapshots else 'Not archived'}")
                  return has_snapshots
              except Exception as e:
                  print(f"Error checking archive status for {url}: {e}")
                  return False
          
          # Function to archive a URL using the Internet Archive API
          def archive_url(url):
              try:
                  print(f"Archiving URL: {url}")
                  response = requests.get(f"https://web.archive.org/save/{url}", timeout=60)
                  success = response.status_code in [200, 201, 202, 203]
                  print(f"Archive result for {url}: {'Success' if success else 'Failed'} (Status: {response.status_code})")
                  return success
              except Exception as e:
                  print(f"Error archiving {url}: {e}")
                  return False
          
          # Function to recursively scrape Mecabricks for URLs
          def scrape_mecabricks(start_url="https://www.mecabricks.com", max_urls=500, visited=None):
              if visited is None:
                  visited = set()
              
              urls_to_archive = set()
              urls_to_visit = [start_url]
              
              while urls_to_visit and len(urls_to_archive) < max_urls and len(visited) < max_urls * 2:
                  current_url = urls_to_visit.pop(0)
                  
                  if current_url in visited:
                      continue
                      
                  visited.add(current_url)
                  print(f"Scraping: {current_url}")
                  
                  try:
                      headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                      response = requests.get(current_url, timeout=20, headers=headers)
                      soup = BeautifulSoup(response.text, 'html.parser')
                      
                      # Add the current URL to our archive list if it's not already archived
                      if current_url not in archived_urls and "mecabricks.com" in current_url:
                          urls_to_archive.add(current_url)
                      
                      # Find all links
                      for link in soup.find_all('a', href=True):
                          href = link['href']
                          
                          # Convert relative URLs to absolute
                          if href.startswith('/'):
                              href = f"https://www.mecabricks.com{href}"
                          elif not href.startswith(('http://', 'https://')):
                              continue
                          
                          # Only follow Mecabricks links
                          if "mecabricks.com" in href and href not in visited:
                              urls_to_visit.append(href)
                      
                      # Respect the site by waiting between requests
                      time.sleep(random.uniform(1, 3))
                      
                  except Exception as e:
                      print(f"Error scraping {current_url}: {e}")
                  
                  # Limit the number of URLs we collect
                  if len(urls_to_archive) >= max_urls:
                      break
              
              return list(urls_to_archive)
          
          # Function to commit changes with retries
          def commit_changes():
              for attempt in range(10):
                  try:
                      # Pull latest changes first
                      subprocess.run(['git', 'pull', '--rebase'], check=True)
                      
                      # Sort URLs in files
                      with open('archived_urls.txt', 'r') as f:
                          archived_list = sorted(list(set(line.strip() for line in f if line.strip())))
                      
                      with open('pasted_urls.txt', 'r') as f:
                          pasted_list = sorted(list(set(line.strip() for line in f if line.strip())))
                      
                      # Write back sorted lists
                      with open('archived_urls.txt', 'w') as f:
                          for url in archived_list:
                              f.write(f"{url}\n")
                      
                      with open('pasted_urls.txt', 'w') as f:
                          for url in pasted_list:
                              f.write(f"{url}\n")
                      
                      # Commit and push
                      subprocess.run(['git', 'add', 'archived_urls.txt', 'pasted_urls.txt'], check=True)
                      
                      # Only commit if there are changes
                      result = subprocess.run(['git', 'diff', '--staged', '--quiet'], check=False)
                      if result.returncode != 0:  # Changes exist
                          subprocess.run(['git', 'commit', '-m', 'Update archived URLs [skip ci]'], check=True)
                          subprocess.run(['git', 'push'], check=True)
                          print("Successfully committed and pushed changes")
                      else:
                          print("No changes to commit")
                      
                      return True  # Success
                  except Exception as e:
                      print(f"Commit attempt {attempt+1} failed: {e}")
                      time.sleep(random.uniform(2, 5))  # Wait before retry
              
              print("Failed to commit changes after 10 attempts")
              return False
          
          # Main processing logic
          archived_count = 0
          max_archives = 100
          remaining_urls = []
          
          # Process URLs from pasted_urls.txt first
          if pasted_urls:
              print("Processing URLs from pasted_urls.txt...")
              for url in pasted_urls:
                  if archived_count >= max_archives:
                      print(f"Reached maximum archive count of {max_archives}")
                      remaining_urls.append(url)
                      continue
                  
                  if url in archived_urls:
                      print(f"URL already in archived list: {url}")
                      continue
                  
                  if is_archived(url):
                      print(f"URL already archived in Internet Archive: {url}")
                      archived_urls.add(url)
                      archived_count += 1
                  else:
                      if archive_url(url):
                          archived_urls.add(url)
                          archived_count += 1
                          print(f"Successfully archived: {url}")
                          # Wait to avoid rate limiting
                          time.sleep(random.uniform(2, 5))
                      else:
                          remaining_urls.append(url)
                          print(f"Failed to archive: {url}")
                          time.sleep(random.uniform(1, 3))
          
          # If we haven't reached our quota and no URLs in pasted_urls.txt, scrape Mecabricks
          if archived_count < max_archives and not pasted_urls:
              print("No URLs in pasted_urls.txt. Scraping Mecabricks...")
              mecabricks_urls = scrape_mecabricks(max_urls=max_archives*2)
              
              for url in mecabricks_urls:
                  if archived_count >= max_archives:
                      print(f"Reached maximum archive count of {max_archives}")
                      remaining_urls.append(url)
                      break
                  
                  if url in archived_urls:
                      print(f"URL already in archived list: {url}")
                      continue
                  
                  if is_archived(url):
                      print(f"URL already archived in Internet Archive: {url}")
                      archived_urls.add(url)
                      archived_count += 1
                  else:
                      if archive_url(url):
                          archived_urls.add(url)
                          archived_count += 1
                          print(f"Successfully archived: {url}")
                          # Wait to avoid rate limiting
                          time.sleep(random.uniform(2, 5))
                      else:
                          remaining_urls.append(url)
                          print(f"Failed to archive: {url}")
                          time.sleep(random.uniform(1, 3))
          
          # Save updated archived_urls.txt
          with open('archived_urls.txt', 'w') as f:
              for url in sorted(archived_urls):
                  f.write(f"{url}\n")
          
          # Save remaining URLs back to pasted_urls.txt
          with open('pasted_urls.txt', 'w') as f:
              for url in sorted(remaining_urls):
                  f.write(f"{url}\n")
          
          print(f"Archived {archived_count} URLs in this run")
          print(f"Total archived URLs: {len(archived_urls)}")
          print(f"Remaining URLs to process: {len(remaining_urls)}")
          
          # Commit changes to the repository
          commit_changes()
          EOF

      - name: Configure Git
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"