name: Archive URLs

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual triggering
  push:  # Add push trigger
  

jobs:
  archive-urls:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Create archive script
        run: |
          cat > archive_url.sh << 'EOF'
          #!/bin/bash
          url=$1
          
          # Try to archive the URL
          response=$(curl -s -w "%{http_code}" -o /dev/null "https://web.archive.org/save/$url")
          
          # Check if archiving was successful (2xx status code)
          if [[ $response =~ ^2[0-9][0-9]$ ]]; then
            echo "SUCCESS"
          else
            echo "FAILED: $response"
          fi
          EOF
          
          chmod +x archive_url.sh

      - name: Process URLs
        id: process_urls
        run: |
          python - << 'EOF'
          import os
          import time
          import requests
          import subprocess
          from bs4 import BeautifulSoup
          
          # Initialize sets for tracking URLs
          archived_urls = set()
          urls_to_process = set()
          
          # Load existing archived URLs
          if os.path.exists('archived_urls.txt'):
              with open('archived_urls.txt', 'r') as f:
                  archived_urls = set(line.strip() for line in f if line.strip())
          
          # Load URLs from pasted_urls.txt
          if os.path.exists('pasted_urls.txt'):
              with open('pasted_urls.txt', 'r') as f:
                  urls_to_process = set(line.strip() for line in f if line.strip())
          
          # Function to check if URL is already archived in Internet Archive
          def is_archived(url):
              try:
                  response = requests.get(f"https://archive.org/wayback/available?url={url}", timeout=10)
                  data = response.json()
                  return len(data.get('archived_snapshots', {})) > 0
              except Exception as e:
                  print(f"Error checking archive status for {url}: {e}")
                  return False
          
          # Function to archive a URL using the bash script
          def archive_url(url):
              try:
                  result = subprocess.run(['./archive_url.sh', url], 
                                         capture_output=True, text=True, check=True)
                  return "SUCCESS" in result.stdout
              except subprocess.CalledProcessError as e:
                  print(f"Error archiving {url}: {e}")
                  return False
          
          # Function to scrape Mecabricks for new URLs
          def scrape_mecabricks():
              new_urls = set()
              try:
                  response = requests.get("https://www.mecabricks.com/en/models/discover", timeout=15)
                  soup = BeautifulSoup(response.text, 'html.parser')
                  
                  # Find model links
                  for link in soup.find_all('a', href=True):
                      href = link['href']
                      if '/en/models/' in href and not href.startswith('http'):
                          full_url = f"https://www.mecabricks.com{href}"
                          if full_url not in archived_urls and full_url not in urls_to_process:
                              new_urls.add(full_url)
              except Exception as e:
                  print(f"Error scraping Mecabricks: {e}")
              
              return new_urls
          
          # Main processing logic
          archived_count = 0
          max_archives = 100
          
          # Process URLs from pasted_urls.txt first
          remaining_urls = set()
          for url in urls_to_process:
              if archived_count >= max_archives:
                  remaining_urls.add(url)
                  continue
                  
              if url in archived_urls:
                  print(f"URL already in archived list: {url}")
                  continue
                  
              if is_archived(url):
                  print(f"URL already archived in Internet Archive: {url}")
                  archived_urls.add(url)
                  archived_count += 1
              else:
                  print(f"Archiving URL: {url}")
                  if archive_url(url):
                      archived_urls.add(url)
                      archived_count += 1
                      print(f"Successfully archived: {url}")
                  else:
                      remaining_urls.add(url)
                      print(f"Failed to archive: {url}")
              
              # Avoid rate limiting
              time.sleep(2)
          
          # If we haven't reached our quota, scrape Mecabricks for more URLs
          if archived_count < max_archives:
              print("Scraping Mecabricks for additional URLs...")
              mecabricks_urls = scrape_mecabricks()
              
              for url in mecabricks_urls:
                  if archived_count >= max_archives:
                      break
                      
                  if url in archived_urls:
                      continue
                      
                  if is_archived(url):
                      print(f"URL already archived in Internet Archive: {url}")
                      archived_urls.add(url)
                      archived_count += 1
                  else:
                      print(f"Archiving URL: {url}")
                      if archive_url(url):
                          archived_urls.add(url)
                          archived_count += 1
                          print(f"Successfully archived: {url}")
                      
                  # Avoid rate limiting
                  time.sleep(2)
          
          # Save updated archived_urls.txt
          with open('archived_urls.txt', 'w') as f:
              for url in sorted(archived_urls):
                  f.write(f"{url}\n")
          
          # Save remaining URLs back to pasted_urls.txt
          with open('pasted_urls.txt', 'w') as f:
              for url in sorted(remaining_urls):
                  f.write(f"{url}\n")
          
          print(f"Archived {archived_count} URLs in this run")
          print(f"Total archived URLs: {len(archived_urls)}")
          print(f"Remaining URLs to process: {len(remaining_urls)}")
          EOF

      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add archived_urls.txt pasted_urls.txt
          git diff --staged --quiet || git commit -m "Update archived URLs [skip ci]"
          git push
