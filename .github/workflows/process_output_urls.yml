name: Archive URLs to Wayback Machine (Balanced Parallel)
on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      total_urls: ${{ steps.split.outputs.total_urls }}
    steps:
      - uses: actions/checkout@v4
      
      - id: split
        name: Prepare and split URLs
        run: |
          if [ ! -f output_urls.txt ]; then
            echo "output_urls.txt not found"
            exit 1
          fi
          
          # Remove duplicates and sort
          sort -u output_urls.txt -o output_urls.txt
          
          total_urls=$(wc -l < output_urls.txt)
          echo "total_urls=$total_urls" >> $GITHUB_OUTPUT
          
          # Create split directory
          mkdir -p splits
          
          # Split into exactly 20 balanced chunks (round-robin)
          split -n l/20 output_urls.txt splits/batch_
          
          # Rename split files to sequential numbering
          i=1
          for file in splits/batch_*; do
            mv "$file" "splits/batch_$(printf "%02d" $i)"
            ((i++))
          done
          
          # Commit the splits
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add splits/
          git commit -m "Create URL batches" || echo "No changes to commit"
          git push

  archive:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        job_id: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
      max-parallel: 20
      fail-fast: false
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Process URL batch
        env:
          JOB_ID: ${{ matrix.job_id }}
        run: |
          batch_file="splits/batch_$(printf "%02d" $JOB_ID)"
          
          # Check if batch exists and has content
          if [ ! -f "$batch_file" ] || [ ! -s "$batch_file" ]; then
            echo "No URLs to process in batch $JOB_ID"
            exit 0
          fi
          
          # Atomic file lock
          if ! mv "$batch_file" "$batch_file.processing" 2>/dev/null; then
            echo "Batch $JOB_ID already being processed"
            exit 0
          fi
          
          echo "Processing batch $JOB_ID ($(wc -l < "$batch_file.processing") URLs)"
          while IFS= read -r url; do
            if [ -n "$url" ]; then
              echo "Archiving: $url"
              curl -sS -X POST "https://web.archive.org/save/$url" >/dev/null
              sleep 3 # Rate limit
            fi
          done < "$batch_file.processing"
          
          # Cleanup processed batch
          rm "$batch_file.processing"
          
          # Update git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -u
          if ! git commit -m "Processed batch $JOB_ID"; then
            echo "No changes to commit"
          fi
          git push

  cleanup:
    needs: [archive]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Consolidate remaining URLs
        run: |
          # Combine any remaining batches
          find splits/ -type f -name 'batch_*' ! -name '*.processing' -exec cat {} + > output_urls.txt
          find splits/ -type f -name 'batch_*.processing' -exec cat {} + >> output_urls.txt
          
          # Deduplicate again
          sort -u output_urls.txt -o output_urls.txt
          
          # Cleanup
          rm -rf splits/
          date > last_archive.txt
          
          # Commit results
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          git commit -m "Cleanup remaining URLs" || echo "No changes to commit"
          git push
