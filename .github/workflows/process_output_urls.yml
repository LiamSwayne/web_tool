name: Archive URLs to Wayback Machine (Non-Commit Splits)
on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      total_urls: ${{ steps.split.outputs.total_urls }}
    steps:
      - uses: actions/checkout@v4
      
      - id: split
        name: Prepare URLs
        run: |
          # Deduplicate and sort URLs
          sort -u output_urls.txt -o output_urls.txt
          
          # Split into 20 balanced chunks
          mkdir -p splits
          split -n l/20 output_urls.txt splits/batch_
          
          # Rename files sequentially
          i=1
          for f in splits/batch_*; do
            mv "$f" "splits/batch_$(printf "%02d" $i)"
            ((i++))
          done
          
          # Output total URLs count
          echo "total_urls=$(wc -l < output_urls.txt)" >> $GITHUB_OUTPUT
      
      - name: Upload Splits
        uses: actions/upload-artifact@v4
        with:
          name: url-splits
          path: splits/

  archive:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        job_id: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
      max-parallel: 20
      fail-fast: false
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Get Splits
        uses: actions/download-artifact@v4
        with:
          name: url-splits
          path: splits/
      
      - name: Process Batch
        env:
          JOB_ID: ${{ matrix.job_id }}
        run: |
          batch_file="splits/batch_$(printf "%02d" $JOB_ID)"
          processed_file="processed_$JOB_ID.txt"
          
          # Process URLs
          if [ -f "$batch_file" ]; then
            while IFS= read -r url; do
              if [ -n "$url" ]; then
                echo "Archiving: $url"
                if curl -sS -X POST "https://web.archive.org/save/$url" >/dev/null; then
                  echo "$url" >> "$processed_file"
                  echo "Successfully archived: $url"
                  sleep 2  # Rate limit
                else
                  echo "Failed to archive: $url"
                fi
              fi
            done < "$batch_file"
          fi
          
          # Upload successful archives
          if [ -f "$processed_file" ]; then
            mv "$processed_file" "processed_urls_$JOB_ID.txt"
          else
            touch "processed_urls_$JOB_ID.txt"
          fi
      
      - name: Upload Processed
        uses: actions/upload-artifact@v4
        with:
          name: processed-${{ matrix.job_id }}
          path: processed_urls_${{ matrix.job_id }}.txt

  cleanup:
    needs: [archive]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Collect Processed
        run: |
          mkdir -p processed
          for job in {1..20}; do
            gh run download $GITHUB_RUN_ID -n processed-$job -D processed/ || true
          done
          cat processed/*.txt | sort -u > all_processed.txt
      
      - name: Update URL List
        run: |
          # Sort both files before using comm
          sort -o output_urls.txt output_urls.txt
          sort -o all_processed.txt all_processed.txt
          
          # Remove successfully archived URLs
          comm -23 output_urls.txt all_processed.txt > remaining_urls.txt
          mv remaining_urls.txt output_urls.txt
          date > last_run.txt
      
      - name: Commit Changes
        run: |
          git config --local user.email "actions@github.com"
          git config --local user.name "URL Archiver"
          git add output_urls.txt last_run.txt
          if git commit -m "Update URLs ($(date +'%Y-%m-%d %H:%M'))"; then
            git push
          else
            echo "No changes to commit"
          fi
