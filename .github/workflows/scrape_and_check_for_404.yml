name: scrape and check for 404
on:
  workflow_dispatch:  # Allows manual triggering
  schedule:
    - cron: '0 * * * *'  # Runs every hour
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
      with:
        fetch-depth: 0  # Fetch all history for all branches and tags
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 fake-useragent
    - name: Run URL scraper
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: python scrape_and_check_for_404.py
    - name: Commit and push if changed
      run: |
        git config --global user.name 'GitHub Action'
        git config --global user.email 'action@github.com'
        git fetch origin main
        git checkout main
        git pull origin main
        if git diff --name-only origin/main | grep -q "source_urls.txt"; then
          git checkout origin/main -- source_urls.txt
        fi
        git add output_urls.txt
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update scraped URLs"; git push)
